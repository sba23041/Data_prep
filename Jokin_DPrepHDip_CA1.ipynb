{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfc01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to be used \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns #visualisation\n",
    "import matplotlib.pyplot as plt #visualisation\n",
    "%matplotlib inline\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd2caeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APS Failure Set successfully imported into a dataframe\n"
     ]
    }
   ],
   "source": [
    "# Import the file into a dataframe \n",
    "\n",
    "try:\n",
    "    aps_failure_set=pd.read_csv(r\"\\\\sedna\\jokin.ormazabal$\\$Profile\\Desktop\\Master\\Data Prep\\aps_failure_set.csv\")\n",
    "    print(\"APS Failure Set successfully imported into a dataframe\")\n",
    "except:\n",
    "    print(\"Something went wrong importing the APS Failure Set\")\n",
    "\n",
    "#As comma is the defaulted delimiter, we dont need to specify it\n",
    "\n",
    "#try to do it trough a url\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983c192",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "######  Based on the explanation received, we know the data divides between the errors relating APS and not, \n",
    "so we divide the main Dataframe into 2 different ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e1c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failures=aps_failure_set.loc[aps_failure_set['class']=='pos']\n",
    "not_aps_failures=aps_failure_set.loc[aps_failure_set['class']=='neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40365c1",
   "metadata": {},
   "source": [
    "###### EXPLANATORY DATA ANALYSIS FOR APS RELATED FAILURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b82d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The APS failures dataset has NO na values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "na_values_aps_failures = aps_failures.isna().sum()\n",
    "na_aps = na_values_aps_failures.to_numpy()\n",
    "\n",
    "if np.sum(na_aps) == 0:\n",
    "    print('The APS failures dataset has NO na values')\n",
    "else:\n",
    "    print('The APS failures dataset has na values')\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2300e2",
   "metadata": {},
   "source": [
    "######  We know by looking at the data that it actually has na values but the function is not catching them. \n",
    "This happens because the na is a string and for python the value is actually not null. \n",
    "It is important to always manually review the data and check that the output from the code make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c635a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The APS failures dataset has values equal to na\n"
     ]
    }
   ],
   "source": [
    "# We will now look for values equal to 'na'\n",
    "\n",
    "na_values_aps_failures=aps_failures.isin(['na']) #creates a df with True for values=='na' & false for values !='na'\n",
    "\n",
    "if (na_values_aps_failures.any().any())==True: \n",
    "    print('The APS failures dataset has values equal to na')\n",
    "else:\n",
    "     print('The APS failures dataset has NO values equal to na')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9b74c",
   "metadata": {},
   "source": [
    "######  As we suspected, we actually have null values. \n",
    "These values will need to be excluded from any statistical analysis and python has ways to do so. \n",
    "The best method will be to convert those na values into propper NaN's so that the code recognises them latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88e70769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The APS failures dataset has na values\n"
     ]
    }
   ],
   "source": [
    "aps_failures=aps_failures.replace(\"na\",np.nan) \n",
    "aps_failures=aps_failures.replace(\"\",np.nan) # In case there are any blank values, we convert them as NaN as well\n",
    "\n",
    "na_aps = na_values_aps_failures.to_numpy()\n",
    "\n",
    "if np.sum(na_aps) == 0:\n",
    "    print('The APS failures dataset has NO na values')\n",
    "else:\n",
    "    print('The APS failures dataset has na values')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da11457",
   "metadata": {},
   "source": [
    "######  Now that we identified the existance of NaN values we will contextualize them and decide if we can drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03bc4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 171)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aps_failures.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0fda9",
   "metadata": {},
   "source": [
    "######  We know that we have 1000 rows and 171 columns. So now It will interesting to see which columns have a great % of nulls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d13e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_percentages=(aps_failures.isnull().sum()/1000*100)\n",
    "null_percentages = aps_failures.isnull().mean() * 100 # we can also do it with the mean attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e44876d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        High_Null\n",
      "class         0.0\n",
      "aa_000        0.0\n",
      "ab_000       77.1\n",
      "ac_000       46.2\n",
      "ad_000       64.5\n",
      "...           ...\n",
      "ee_007        0.5\n",
      "ee_008        0.5\n",
      "ee_009        0.5\n",
      "ef_000       37.7\n",
      "eg_000       37.7\n",
      "\n",
      "[171 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "High_null_aps_failures = pd.DataFrame(null_percentages, columns=['High_Null']) #Data frame with the % of null values by column\n",
    "print (High_null_aps_failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d27355c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "High_null_aps_failures.reset_index(drop=False, inplace=True) #I need to add an index as the sensor names where not counting as a propper column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "affaa5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (High_null_aps_failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21297a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "High_null_aps_failures=High_null_aps_failures[High_null_aps_failures.iloc[:,1]>20] #Select the sensors with greater than 20% null values\n",
    "\n",
    "High_null_aps_failures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcbc651",
   "metadata": {},
   "source": [
    "######  We see that there are 60 sensors with more than 20% of null values. I consider that number too high to analyse them\n",
    "\n",
    "###### I'll drop all those columns to focus only on the most reliable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c9cd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_sensors=High_null_aps_failures.iloc[:,0].tolist() #get the name of the faulty sensors on a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a6a11d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 111)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aps_failures=aps_failures.drop(faulty_sensors,axis=1)\n",
    "aps_failures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971a794",
   "metadata": {},
   "source": [
    "######  We droped what I considered faulty sensors by the high null values, but still we have nulls on the other sensors.\n",
    "###### The existance of this nulls makes the sample incosistant as we have more records for some sensors. \n",
    "###### We will drop the rows with null values to make the data set equal for every sensor type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65b966b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(861, 111)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aps_failures = aps_failures.dropna()\n",
    "aps_failures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4521272",
   "metadata": {},
   "source": [
    "###### We have cleaned our dataset from nulls and \"faulty\" data, but still we have a high number of different sensors to analyse\n",
    "\n",
    "###### As each number represents the Component Sensor result we cant group the data with the mean. (I assume the number is the type of error, not the number of errors)\n",
    "\n",
    "###### I'll use the mode to detect the most repeated error type by sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c649d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_failures.set_index(aps_failures.columns[0],inplace=True) # To calculate the mode I need to exclude the first column, so I'll make it the index\n",
    "\n",
    "aps_failures=aps_failures.astype(float) # Convert all the df to float type. It got converted to int with so many cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c200cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted = aps_failures.melt(var_name='Sensor', value_name='Most Common Error') # Group the number of errors by Sensor type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4003a4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Sensor  Most Common Error\n",
      "0      aa_000           453236.0\n",
      "1      aa_000          1056758.0\n",
      "2      aa_000           361638.0\n",
      "3      aa_000           791254.0\n",
      "4      aa_000          1053152.0\n",
      "...       ...                ...\n",
      "94705  ee_009            10790.0\n",
      "94706  ee_009             2458.0\n",
      "94707  ee_009                0.0\n",
      "94708  ee_009                0.0\n",
      "94709  ee_009                0.0\n",
      "\n",
      "[94710 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print (df_melted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd0067c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sensor</th>\n",
       "      <th>Most Common Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa_000</td>\n",
       "      <td>181460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ag_000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ag_001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ag_002</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ag_003</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>ee_005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ee_006</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>ee_007</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>ee_008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>ee_009</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sensor  Most Common Error\n",
       "0    aa_000           181460.0\n",
       "1    ag_000                0.0\n",
       "2    ag_001                0.0\n",
       "3    ag_002                0.0\n",
       "4    ag_003                0.0\n",
       "..      ...                ...\n",
       "105  ee_005                0.0\n",
       "106  ee_006                0.0\n",
       "107  ee_007                0.0\n",
       "108  ee_008                0.0\n",
       "109  ee_009                0.0\n",
       "\n",
       "[110 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the mode of each column\n",
    "mode = aps_failures.mode().iloc[0]\n",
    "\n",
    "# Convert to DataFrame and reset index\n",
    "df_aps_modes = pd.DataFrame(mode).reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "df_aps_modes.columns = ['Sensor', 'Most Common Error']\n",
    "\n",
    "df_aps_modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576549e0",
   "metadata": {},
   "source": [
    "###### Select only sensors with an error. (I assume that the 0 means no error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78f0dc28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sensor  Most Common Error\n",
      "0   aa_000          181460.00\n",
      "11  ah_000        14818484.00\n",
      "16  an_000        16230962.00\n",
      "17  ao_000        13692914.00\n",
      "18  ap_000         6904476.00\n",
      "19  aq_000         3618134.00\n",
      "53  bb_000        24654966.00\n",
      "54  bg_000        14818484.00\n",
      "55  bh_000          363444.00\n",
      "56  bi_000         1550620.00\n",
      "57  bj_000         5339498.00\n",
      "58  bk_000          387860.00\n",
      "59  bl_000          393520.00\n",
      "60  bm_000          424600.00\n",
      "61  bn_000          438120.00\n",
      "62  bo_000         1310700.00\n",
      "63  bp_000         1310700.00\n",
      "64  bq_000         1310700.00\n",
      "65  br_000         1310700.00\n",
      "66  bs_000           22880.00\n",
      "67  bt_000          181459.46\n",
      "68  bu_000        24654966.00\n",
      "69  bv_000        24654966.00\n",
      "70  bx_000             226.00\n",
      "72  cb_000           42860.00\n",
      "74  cd_000         1209600.00\n",
      "75  ci_000        14244931.20\n",
      "77  ck_000        10022479.68\n",
      "88  cq_000        24654966.00\n",
      "99  dn_000          242252.00\n"
     ]
    }
   ],
   "source": [
    "df_aps_modes=df_aps_modes[df_aps_modes['Most Common Error'] != 0]\n",
    "\n",
    "print (df_aps_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9651f0ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aps_modes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5217240",
   "metadata": {},
   "source": [
    "###### We get the mode again to see which is the most common failure id for the APS related issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af54434d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1310700.0\n",
      "1    24654966.0\n",
      "Name: Most Common Error, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "mode = df_aps_modes['Most Common Error'].mode()\n",
    "print(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397aee1",
   "metadata": {},
   "source": [
    "######  At this stage, we can derive some conclusions for the APS related failures. \n",
    "    - 60 out of 171 were considered failed sensors because 20% or more than their results were null (35%)\n",
    "    - 139 records out 1000 needed to be excluded as one of the sensors triggered null value (14%)\n",
    "    - 30 from the 111 sensors that were took into the analysis tiggered some kind of error (27%)\n",
    "    - 1310700 & 24654966 are the most common failures related with the APS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372435d6",
   "metadata": {},
   "source": [
    "##### EXPLANATORY DATA ANALYSIS FOR NOT APS RELATED FAILURES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8766e702",
   "metadata": {},
   "source": [
    "###### We replace na for nan so the code can detect them as we learned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dee7ff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Not APS failures dataset has na values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#not_aps_failures=not_aps_failures.drop(['class'],axis=1)\n",
    "\n",
    "not_aps_failures=not_aps_failures.replace(\"na\",np.nan) \n",
    "not_aps_failures=not_aps_failures.replace(\"\",np.nan) \n",
    "\n",
    "if not_aps_failures.isnull().values.any():\n",
    "    print('The Not APS failures dataset has na values')\n",
    "else:\n",
    "    print('The Not APS failures dataset has NO na values')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ccd2e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2ec5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_aps_failures=not_aps_failures.drop(['class'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac415c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The APS failures dataset has na values\n"
     ]
    }
   ],
   "source": [
    "aps_failures=aps_failures.replace(\"na\",np.nan) \n",
    "aps_failures=aps_failures.replace(\"\",np.nan) # In case there are any blank values, we convert them as NaN as well\n",
    "\n",
    "na_aps = na_values_aps_failures.to_numpy()\n",
    "\n",
    "if np.sum(na_aps) == 0:\n",
    "    print('The APS failures dataset has NO na values')\n",
    "else:\n",
    "    print('The APS failures dataset has na values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b5e1880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['aa_000', 'ab_000', 'ac_000', 'ad_000', 'ae_000', 'af_000', 'ag_000',\n",
      "       'ag_001', 'ag_002', 'ag_003',\n",
      "       ...\n",
      "       'ee_002', 'ee_003', 'ee_004', 'ee_005', 'ee_006', 'ee_007', 'ee_008',\n",
      "       'ee_009', 'ef_000', 'eg_000'],\n",
      "      dtype='object', length=170)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63f7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
